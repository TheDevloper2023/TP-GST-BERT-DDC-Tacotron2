{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tacotron2-TP-GST-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oROxV56Ojdpo"
      },
      "source": [
        "Copyright Â©  2021  MIT License | Sapozhnikov Andrey Mikhailovich  \n",
        "* * *\n",
        "**Tacotron2 + TP-GST + BERT**  \n",
        "this notebook shows train and inference usage of realization  \n",
        "[Text-predicting Global Style Tokens for Tacotron2](https://arxiv.org/pdf/1808.01410.pdf) with [BERT](https://m.habr.com/ru/company/sberdevices/blog/548812/)  \n",
        "\n",
        "**Model repository**: https://github.com/lightbooster/TP-GST-BERT-Tacotron2.git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vqX2cSatZC3",
        "outputId": "8ffaf75a-8f34-4863-fb3f-3d2e1dbc50a1"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 14 10:52:25 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMZaONSEcj6E"
      },
      "source": [
        "# Tacotron2 TP-GST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wED9pnYqD7l"
      },
      "source": [
        "## Prepair for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GlQpcbbiCEq"
      },
      "source": [
        "Mount Google Drive to store all checkpoints and datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG7nvAsQkZvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f216c13-aef8-431e-ae15-b0e2a76ea34d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZkZbi9NqMgg"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx6TC90xd61S"
      },
      "source": [
        "Download M-AILABS Russian dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl8yezt3eFgO"
      },
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "!wget http://www.caito.de/data/Training/stt_tts/ru_RU.tgz\n",
        "!tar -zxvf ru_RU.tgz\n",
        "# Rename directory\n",
        "!mv ru_RU M_AILABS_Ru"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4NUGxGwRI8q"
      },
      "source": [
        "### Preprocess Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWOWZHleVY8C"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqU03n2uLhG7"
      },
      "source": [
        "\"\"\"\n",
        "Move all wav file to the common folder\n",
        "Concat all text files into one\n",
        "\"\"\"\n",
        "def refactor_M_AILABS(sex='female', speaker_index=0\n",
        "                      , dataset_dir=\"/content/drive/MyDrive/M_AILABS_Ru/\"\n",
        "                      , out_dir=\"/content/drive/MyDrive/M_AILABS_Ru/\"\n",
        "                      , move_files=True\n",
        "                      ):\n",
        "  print(\" *** Start refactoring... ***\")\n",
        "  path = f\"{dataset_dit}{''if dataset_dit[-1] == '/' else '/'}by_book/{sex}\"\n",
        "  %cd $path\n",
        "  speakers = !ls\n",
        "  books_path = f\"{path}/{speakers[speaker_index]}\"\n",
        "  %cd $books_path\n",
        "  books_folders = !ls -d */ | cut -f1 -d'/'\n",
        "  path = f\"{out_dir}{''if out_dir[-1] == '/' else '/'}\"\n",
        "  %cd $path\n",
        "  !mkdir wavs\n",
        "  wavs_path = f\"{path}wavs/\"\n",
        "  !touch mlbs_audio_text.txt\n",
        "  for book in books_folders:\n",
        "    cur_wavs_path = f\"{books_path}/{book}/wavs/*\"\n",
        "    cur_text_path = f\"{books_path}/{book}/metadata.csv\"\n",
        "    !cat $cur_text_path >> mlbs_audio_text.txt\n",
        "    if move_files:\n",
        "      !mv -v $cur_wavs_path $wavs_path\n",
        "  print(\" *** Refactoring finished! ***\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMG7RXjQgTDE"
      },
      "source": [
        "refactor_M_AILABS()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpPHm_uWJbkC"
      },
      "source": [
        "\"\"\"\n",
        "M-Ailabs dataset has 2 strings of audio trascription -\n",
        "raw and normalized\n",
        "This function chooose one, based on argument \n",
        "and add wav file full path\n",
        "\"\"\"\n",
        "def transform_m_ailabs_text(file_name=\"mlbs_audio_text.txt\",\n",
        "                            new_file_name=\"mlbs_audio_text_transformed.txt\",\n",
        "                            dataset_dir=\"/content/drive/MyDrive/M_AILABS_Ru/\",\n",
        "                            raw_data=False):\n",
        "  write_file = open(dataset_dir+new_file_name, 'w', encoding='utf-8')\n",
        "  with open(dataset_dir+file_name, 'r', encoding='utf-8') as read_file:\n",
        "    for line in read_file:\n",
        "      line = line.split('|')\n",
        "      wav_file_path = f\"{dataset_dir}wavs/{line[0]}.wav\"\n",
        "      new_line = f\"{wav_file_path}|{line[1] if raw_data else line[2]}\"\n",
        "      write_file.write(new_line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssULM4fHMf1S"
      },
      "source": [
        "transform_m_ailabs_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTgvTMpXJHvk"
      },
      "source": [
        "\"\"\"\n",
        "We should split data into 3 sets:\n",
        "'train', 'test' and 'eval'\n",
        "\"\"\"\n",
        "def split_dataset(test_size=0.05, eval_size=0.01,\n",
        "                  dataset_dir=\"/content/drive/MyDrive/M_AILABS_Ru/\",\n",
        "                  source_file_name=\"mlbs_audio_text_transformed.txt\",\n",
        "                  output_file_name_base=\"mlbs_audio_text\",\n",
        "                  shuffle=True):\n",
        "  if test_size + eval_size >= 1:\n",
        "    print(\"ERROR: (test_size + eval_size) must be less 1\")\n",
        "    return\n",
        "  \n",
        "  data = []\n",
        "  with open(f\"{dataset_dir}{source_file_name}\",\n",
        "            'r', encoding='utf-8') as read_file:\n",
        "    for line in read_file:\n",
        "      data.append(line)\n",
        "  \n",
        "  length = len(data)\n",
        "  test_length = int(length * test_size)\n",
        "  eval_length = int(length * eval_size)\n",
        "  if shuffle:\n",
        "    random.shuffle(data)\n",
        "  test_data = data[: test_length]\n",
        "  eval_data = data[test_length : test_length + eval_length]\n",
        "  train_data = data[test_length + eval_length :]\n",
        "\n",
        "  sets = {'test': test_data, 'eval': eval_data, 'train': train_data}\n",
        "  for name, dataset in sets.items():\n",
        "    with open(f\"{dataset_dir}{output_file_name_base}_{name}.txt\", \n",
        "              'w', encoding='utf-8') as file:\n",
        "      for line in dataset:\n",
        "        file.write(line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KWsdrFDVjlP"
      },
      "source": [
        "split_dataset(shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyxTCy-qqRTi"
      },
      "source": [
        "### Install libs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5il7_ZImEj4"
      },
      "source": [
        "Download BERT checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0fYk_HMlm3e"
      },
      "source": [
        "!wget http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "!tar -xvzf rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "%cd rubert_cased_L-12_H-768_A-12_pt/\n",
        "!mv bert_config.json config.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFirQQzctq1l"
      },
      "source": [
        "Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2kpBbeUdN3c"
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/lightbooster/TP-GST-BERT-Tacotron2.git\n",
        "%cd TP-GST-BERT-Tacotron2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH0DmEU-qm_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53699820-571b-461c-abb8-f933a2c67fe6"
      },
      "source": [
        "!git submodule init; git submodule update"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submodule 'waveglow' (https://github.com/NVIDIA/waveglow.git) registered for path 'waveglow'\n",
            "Cloning into '/content/mellotron/waveglow'...\n",
            "Submodule path 'waveglow': checked out '2fd4e63e2918012f55eac2c8a8e75622a39741be'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frO-aSUHlXIb",
        "outputId": "d2e76674-b712-4462-ec03-ecb4f2a7e58c"
      },
      "source": [
        "!cat requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "matplotlib==2.1.0\n",
            "tensorflow==1.15.2\n",
            "inflect==0.2.5\n",
            "librosa==0.6.0\n",
            "scipy==1.0.0\n",
            "tensorboardX==1.1\n",
            "Unidecode==1.0.22\n",
            "pillow\n",
            "nltk==3.4.5\n",
            "jamo==0.4.1\n",
            "music21\n",
            "transformers==4.6.1"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkfnpIfmtgIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf2b80a1-5b52-4d4a-eb41-39f65b4c2185"
      },
      "source": [
        "!pip install apex\n",
        "!pip install Unidecode\n",
        "!pip install tensorflow==1.15.2\n",
        "!pip install transformers==4.6.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting apex\n",
            "  Downloading https://files.pythonhosted.org/packages/31/b6/923de12ffcc2686157d7f74b96396b87c854eaaeec9d441d120facc2a0e0/apex-0.9.10dev.tar.gz\n",
            "Collecting cryptacular\n",
            "  Downloading https://files.pythonhosted.org/packages/ec/d6/a82d191ec058314b2b7cbee5635150f754ba1c6ffc05387bc9a57efe48b8/cryptacular-1.5.5.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.sqlalchemy\n",
            "  Downloading https://files.pythonhosted.org/packages/59/ab/0a19b1a931ea26346ef803bbf4db84ce8b30f5d64933204e92867c0f3b82/zope.sqlalchemy-1.4-py2.py3-none-any.whl\n",
            "Collecting velruse>=1.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/d9/e18b5c98667c45f5dd1a256d72168ea5ff68f0025fc5b24be010f2696ca3/velruse-1.1.1.tar.gz (709kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 716kB 17.7MB/s \n",
            "\u001b[?25hCollecting pyramid>1.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/05/c75c59a1d8a3e44d3cb7d9ef9969bd4ff6dc18d860519dd1ef0cd547139f/pyramid-2.0-py3-none-any.whl (246kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 256kB 27.9MB/s \n",
            "\u001b[?25hCollecting pyramid_mailer\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/c3/0ce593179a8da8e1ab7fe178b0ae096a046246bd44a5787f72940d6dd5b2/pyramid_mailer-0.15.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from apex) (2.23.0)\n",
            "Collecting wtforms\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/31/614fc7dc7d76005b0acb8c0c8920d962b83d7422b4ba912886dfb63f86ff/WTForms-2.3.3-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 174kB 29.4MB/s \n",
            "\u001b[?25hCollecting wtforms-recaptcha\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/b0/42021ab061b768e3e5f430466219468c2afec99fe706e4340792d7a6fab4/wtforms_recaptcha-0.3.2-py2.py3-none-any.whl\n",
            "Collecting pbkdf2\n",
            "  Downloading https://files.pythonhosted.org/packages/02/c0/6a2376ae81beb82eda645a091684c0b0becb86b972def7849ea9066e3d5e/pbkdf2-1.3.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from cryptacular->apex) (57.0.0)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9 in /usr/local/lib/python3.7/dist-packages (from zope.sqlalchemy->apex) (1.4.15)\n",
            "Collecting zope.interface>=3.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/a7/94e1a92c71436f934cdd2102826fa041c83dcb7d21dd0f1fb1a57f6e0620/zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 256kB 48.5MB/s \n",
            "\u001b[?25hCollecting transaction>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/35/b5cca132f9b364066bea00cbf4ea466b7a15461a609ab9ba8e832e165452/transaction-3.0.1-py2.py3-none-any.whl (47kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 51kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from velruse>=1.0.3->apex) (1.3.0)\n",
            "Collecting anykeystore\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/dc/c4399c0e6b835710763705220f9c37681683f950678db799a5c7eda9e154/anykeystore-0.2.tar.gz\n",
            "Collecting python3-openid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/a5/c6ba13860bdf5525f1ab01e01cc667578d6f1efc8a1dba355700fb04c29b/python3_openid-3.2.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 143kB 49.9MB/s \n",
            "\u001b[?25hCollecting venusian>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/43/92/3d522a710867168ee422a0ffbd712c425ece937aaeec4381497a59e24faf/venusian-3.0.0-py3-none-any.whl\n",
            "Collecting plaster-pastedeploy\n",
            "  Downloading https://files.pythonhosted.org/packages/11/c4/0470056ea324c7a420c22647be512dec1b5e32b1b6e77e27c61838d2811c/plaster_pastedeploy-0.7-py2.py3-none-any.whl\n",
            "Collecting hupper>=1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/cb/d30896ad0d288358bbae60576556df78422179fc08c630f863258a7800ea/hupper-1.10.3-py2.py3-none-any.whl\n",
            "Collecting zope.deprecation>=3.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f9/26/b935bbf9d27e898b87d80e7873a0200cebf239253d0afe7a59f82fe90fff/zope.deprecation-4.4.0-py2.py3-none-any.whl\n",
            "Collecting translationstring>=0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/98/36187601a15e3d37e9bfcf0e0e1055532b39d044353b06861c3a519737a9/translationstring-1.4-py2.py3-none-any.whl\n",
            "Collecting webob>=1.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/9c/e94a9982e9f31fc35cf46cdc543a6c2c26cb7174635b5fd25b0bbc6a7bc0/WebOb-1.8.7-py2.py3-none-any.whl (114kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 122kB 47.7MB/s \n",
            "\u001b[?25hCollecting plaster\n",
            "  Downloading https://files.pythonhosted.org/packages/61/29/3ac8a5d03b2d9e6b876385066676472ba4acf93677acfc7360b035503d49/plaster-1.0-py2.py3-none-any.whl\n",
            "Collecting repoze.sendmail>=4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/98/c5c64dc045b7c45858c391d04673a0f2748acef8e0eea4f2989b22220f97/repoze.sendmail-4.4.1-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 51kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->apex) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->apex) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->apex) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->apex) (2.10)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.7/dist-packages (from wtforms->apex) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex) (4.0.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex) (1.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
            "Collecting PasteDeploy>=2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/0b/d47ea894587f3155f8c4520aa74d57c856189d0bbe27e831881d655a3386/PasteDeploy-2.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex) (3.7.4.3)\n",
            "Building wheels for collected packages: cryptacular\n",
            "  Building wheel for cryptacular (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cryptacular: filename=cryptacular-1.5.5-cp37-abi3-manylinux2010_x86_64.whl size=48277 sha256=1f59cecffc8510f842f42875ee2b07f16c3aa978fd68c87d7df90953be02b4a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/79/bc/1eec7120c3ff9b0a2c7ad94d1626abc3388688e2ed7a45878f\n",
            "Successfully built cryptacular\n",
            "Building wheels for collected packages: apex, velruse, pbkdf2, anykeystore\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.9.10.dev0-cp37-none-any.whl size=46468 sha256=20b9afb8677c8b0ad3147886456ecc2b82e440f933da75e1e88e9763e4795008\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/4c/4b/2990cf86a29c679ae4bd5f4de5723aa8a4af107721089c9a55\n",
            "  Building wheel for velruse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for velruse: filename=velruse-1.1.1-cp37-none-any.whl size=50938 sha256=83336113207f2dfa48b2fac93ef39cf0e0f5a79c487e1f9b2c9045558d52927b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/65/9e/b805aad8ec3a359591c497b257dabe911f305d285b5d8a13cc\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-cp37-none-any.whl size=5104 sha256=895f3fe641b0c820f7887a778b99333c085c60a26467d90ea20647f8dd029ac6\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/62/b9/0bf3a68f2111e169253ec4d2bbdc303c46777b7fc99bbbf230\n",
            "  Building wheel for anykeystore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anykeystore: filename=anykeystore-0.2-cp37-none-any.whl size=17044 sha256=713a4949b13860247cc83e740465493ef11d9239ce9543a79dbdfae373965dbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/a9/b2/f79f84cbee6613c9edce6d98b9e1410c1d41d38953bd94eed2\n",
            "Successfully built apex velruse pbkdf2 anykeystore\n",
            "Installing collected packages: pbkdf2, cryptacular, zope.interface, transaction, zope.sqlalchemy, venusian, PasteDeploy, plaster, plaster-pastedeploy, hupper, zope.deprecation, translationstring, webob, pyramid, anykeystore, python3-openid, velruse, repoze.sendmail, pyramid-mailer, wtforms, wtforms-recaptcha, apex\n",
            "Successfully installed PasteDeploy-2.1.1 anykeystore-0.2 apex-0.9.10.dev0 cryptacular-1.5.5 hupper-1.10.3 pbkdf2-1.3 plaster-1.0 plaster-pastedeploy-0.7 pyramid-2.0 pyramid-mailer-0.15.1 python3-openid-3.2.0 repoze.sendmail-4.4.1 transaction-3.0.1 translationstring-1.4 velruse-1.1.1 venusian-3.0.0 webob-1.8.7 wtforms-2.3.3 wtforms-recaptcha-0.3.2 zope.deprecation-4.4.0 zope.interface-5.4.0 zope.sqlalchemy-1.4\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 245kB 13.8MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.2.0\n",
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/81/84fb7a323f9723f81edfc796d89e89aa95a9446ed7353c144195b3a3a3ba/tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 110.5MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 51kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.34.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 512kB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 3.8MB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=94b6b68f55c956394d7ae5de54104ff162b6f465108d7cb997d12d75489f1237\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n",
            "Collecting transformers==4.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 2.3MB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 3.3MB 50.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 901kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1) (4.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.1) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.1) (3.4.1)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 122kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (57.0.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dn4kR_Bka13"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCuId6vk0wdU",
        "outputId": "5ce3c5fd-0328-41e1-8a68-8d45ddbc337b"
      },
      "source": [
        "%cd /content/TP-GST-BERT-Tacotron2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mellotron\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVNKrX6chFVm"
      },
      "source": [
        "### Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIZGtNfMhIhj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "77b16cf3-253d-4631-c30d-9151aeb02f28"
      },
      "source": [
        "home_dir = \"/content/drive/MyDrive/\"\n",
        "\n",
        "training_files = home_dir + \"M_AILABS_Ru/mlbs_audio_text_train.txt\"\n",
        "validation_files = home_dir + \"M_AILABS_Ru/mlbs_audio_text_eval.txt\"\n",
        "\n",
        "output_directory = home_dir + \"M_AILABS_Ru/checkpoints_GST\"\n",
        "log_directory = \"../logs_GST\"\n",
        "\n",
        "batch_size = 16\n",
        "iters_per_checkpoint = 1000\n",
        "sampling_rate = 16000\n",
        "\n",
        "bert_path = home_dir + \"ModelsCheckpoints/rubert_cased_L-12_H-768_A-12_pt/\"\n",
        "\n",
        "hparams = f\"batch_size={batch_size},\\\n",
        "            iters_per_checkpoint={iters_per_checkpoint},\\\n",
        "            training_files={training_files},\\\n",
        "            validation_files={validation_files},\\\n",
        "            sampling_rate={sampling_rate},\\\n",
        "            bert_checkpoint_path={bert_path}\"\n",
        "\n",
        "hparams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'batch_size=16,            iters_per_checkpoint=1000,            training_files=/content/drive/MyDrive/M_AILABS_Ru/mlbs_audio_text_train.txt,            validation_files=/content/drive/MyDrive/M_AILABS_Ru/mlbs_audio_text_eval.txt,            sampling_rate=16000,            bert_checkpoint_path=/content/drive/MyDrive/ModelsCheckpoints/rubert_cased_L-12_H-768_A-12_pt/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WG_YhAWh3OQ"
      },
      "source": [
        "### Pretrained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_qM0azaHD4G"
      },
      "source": [
        "NVIDEA checkpoint and deleted language weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmo5ccTyhzhp"
      },
      "source": [
        "checkpoint_path = \"../drive/MyDrive/tacotron2_statedict.pt\"\n",
        "!python train.py --output_directory=$output_directory --log_directory=$log_directory --hparams \"$hparams\" -c $checkpoint_path --warm_start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxYb8aq3HAq_"
      },
      "source": [
        "my checkpoint and saved weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "015kGrurHJhh"
      },
      "source": [
        "checkpoint_num = 161000\n",
        "checkpoint_path = f\"{output_directory}/checkpoint_{checkpoint_num}\"\n",
        "!python train.py --output_directory=$output_directory --log_directory=$log_directory --hparams \"$hparams\" -c $checkpoint_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDjCO8rwh96d"
      },
      "source": [
        "### From the start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8iofLnzcvMY"
      },
      "source": [
        "!python train.py --output_directory=$output_directory --log_directory=$log_directory --hparams \"$hparams\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fyvTkM_wMW5"
      },
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTjNjO_9gq-"
      },
      "source": [
        "### Tacotron2 + TP-GST + BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8kpWA35m3yf",
        "outputId": "2a98a6c3-8b59-4d28-f8a8-a1cff71a95e8"
      },
      "source": [
        "%cd /content/TP-GST-BERT-Tacotron2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mellotron\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf83z6dSwNkJ"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "sys.path.append('waveglow/')\n",
        "\n",
        "from itertools import cycle\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.io.wavfile import write\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "from hparams import create_hparams\n",
        "from model import Tacotron2, load_model\n",
        "from layers import TacotronSTFT\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from text import cmudict, text_to_sequence\n",
        "from mellotron_utils import get_data_from_musicxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbmD4ZmeOsD9"
      },
      "source": [
        "def plot_data(data, figsize=(16, 4)):\n",
        "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
        "    for i in range(len(data)):\n",
        "        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
        "                       interpolation='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuUBTOXHUBtO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7043e25d-b9c5-4f33-baac-8d577341907b"
      },
      "source": [
        "checkpoint_dir = \"../drive/MyDrive/M_AILABS_Ru/checkpoints_GST/\"\n",
        "checkpoint_name = \"checkpoint_\"\n",
        "checkpoint_num = 130000\n",
        "checkpoint_path = checkpoint_dir + checkpoint_name + str(checkpoint_num)\n",
        "checkpoint_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'../drive/MyDrive/M_AILABS_Ru/checkpoints_GST/checkpoint_177000'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlxmaJlPOxHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5400b08f-cd7e-45fa-b451-332c1cf9a7ca"
      },
      "source": [
        "hparams = create_hparams()\n",
        "hparams.sampling_rate = 16000\n",
        "hparams.bert_checkpoint_path = bert_path = home_dir + \"ModelsCheckpoints/rubert_cased_L-12_H-768_A-12_pt/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gb62KPnysLT",
        "outputId": "7f23b029-006b-48ed-e2d3-c38236d3f443"
      },
      "source": [
        "model = load_model(hparams)\n",
        "model.load_state_dict(torch.load(checkpoint_path)['state_dict'], strict=False)\n",
        "_ = model.cuda().eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/ModelsCheckpoints/rubert_cased_L-12_H-768_A-12_pt/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "bert from checkpoint\n",
            "---------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcIfaKHvVCMe"
      },
      "source": [
        "### WaveGlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4L14EgJVD6s",
        "outputId": "6a2adc20-9efd-4444-b430-63defa538cab"
      },
      "source": [
        "waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=False)\n",
        "checkpoint = torch.hub.load_state_dict_from_url('https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427')\n",
        "\n",
        "# # Unwrap the DistributedDataParallel module\n",
        "state_dict = {key.replace(\"module.\", \"\"): value for key, value in checkpoint[\"state_dict\"].items()}\n",
        "\n",
        "# # Apply the state dict to the model\n",
        "waveglow.load_state_dict(state_dict)\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to('cuda')\n",
        "_ = waveglow.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/nvidia_DeepLearningExamples_torchhub\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujP1yev8BH8n"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-4hNWB0TCQ0"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWvCbwA_BZio"
      },
      "source": [
        "text = \"ÐÐµ Ð¿Ð¾Ð´ÑÐºÐ°Ð¶Ð¸ÑÐµ ÐºÐ°ÐºÐ¾Ð¹ ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð´ÐµÐ½Ñ?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TFuV51NBLSv",
        "outputId": "266f750f-14d5-4be6-9410-b1750eb22cb1"
      },
      "source": [
        "# preprocessing\n",
        "sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
        "sequence = torch.from_numpy(sequence).to(device='cuda', dtype=torch.int64)\n",
        "sequence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 88,  79,   8,  90,  89,  78,  93,  85,  75, 100,  82,  83,  94,  79,\n",
              "           8,  85,  75,  85,  89,  83,   8,  93,  79,  81,  89,  78,  88,  83,\n",
              "          75,   8,  78,  79,  88,   1,   7]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PiY03B7InmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca7a938-4827-403a-fc1c-56553644e354"
      },
      "source": [
        "# run the models\n",
        "with torch.no_grad():\n",
        "    # here choose the tp-gst model from 'tpse', 'tpse-linear' and 'tpcw'\n",
        "    predicted = model.inference((sequence, text), 'tpse')\n",
        "    mel_outputs, mel_outputs_postnet, _, alignments = predicted\n",
        "    audio = waveglow.infer(mel_outputs_postnet)\n",
        "audio_numpy = audio[0].data.cpu().numpy()\n",
        "rate = 16000\n",
        "mel_outputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 80, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX7wdy8NJjXo"
      },
      "source": [
        "Reference audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aVmfrMSJldQ"
      },
      "source": [
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBeFP_0DJogL"
      },
      "source": [
        "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                    hparams.mel_fmax)\n",
        "\n",
        "def load_mel(path):\n",
        "    audio, sampling_rate = librosa.core.load(path, sr=16000)\n",
        "    audio = torch.from_numpy(audio)\n",
        "    if sampling_rate != hparams.sampling_rate:\n",
        "        raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "            sampling_rate, stft.sampling_rate))\n",
        "    audio_norm = audio.unsqueeze(0)\n",
        "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "    melspec = stft.mel_spectrogram(audio_norm)\n",
        "    melspec = melspec.cuda()\n",
        "    return melspec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY0sTKD3JuQF",
        "outputId": "029905e5-0f1e-403d-9366-3216ed1a74e0"
      },
      "source": [
        "reference_mel = load_mel('/content/drive/MyDrive/M_AILABS_Ru/wavs/chetvero_nischih_s000001.wav')\n",
        "reference_mel.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 80, 639])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWNcLguMG8gm",
        "outputId": "f3cb3b07-ca78-4486-8c77-97dfea442f90"
      },
      "source": [
        "# run the models\n",
        "with torch.no_grad():\n",
        "    mel_outputs, mel_outputs_postnet, _, alignments = model.inference_reference((sequence, reference_mel))\n",
        "    audio = waveglow.infer(mel_outputs_postnet)\n",
        "audio_numpy = audio[0].data.cpu().numpy()\n",
        "rate = 16000\n",
        "mel_outputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 80, 335])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EukVm1BYyTK"
      },
      "source": [
        "Visualize Mels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "416OiX7KQIn2"
      },
      "source": [
        "%matplotlib inline\n",
        "plot_data((mel_outputs.float().data.cpu().numpy()[0],\n",
        "           mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
        "           alignments.float().data.cpu().numpy()[0].T))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MfXQzITYP7x"
      },
      "source": [
        "### Audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGzP-_joAH5O"
      },
      "source": [
        "with torch.no_grad():\n",
        "    audio = waveglow.infer(mel_outputs)\n",
        "audio_numpy = audio[0].data.cpu().numpy()\n",
        "rate = 22500\n",
        "ipd.Audio(audio_numpy, rate=rate)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}